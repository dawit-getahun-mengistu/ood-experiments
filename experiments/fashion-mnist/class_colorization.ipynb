{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('../../experiments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_COLORS = {\n",
    "    0: (255, 0, 0),    # Red - T-shirt/top\n",
    "    1: (0, 255, 0),    # Green - Trouser\n",
    "    2: (0, 0, 255),    # Blue - Pullover\n",
    "    3: (255, 255, 0),  # Yellow - Dress\n",
    "    4: (255, 165, 0),  # Orange - Coat\n",
    "    5: (128, 0, 128),  # Purple - Sandal\n",
    "    6: (0, 255, 255),  # Cyan - Shirt\n",
    "    7: (255, 105, 180),# Pink - Sneaker\n",
    "    8: (128, 128, 128),# Gray - Bag\n",
    "    9: (255, 255, 255) # White - Ankle boot\n",
    "}\n",
    "\n",
    "# Normalize color values to [0,1]\n",
    "CLASS_COLORS = {k: np.array(v) / 255.0 for k, v in CLASS_COLORS.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassColorizeTransform:\n",
    "    def __init__(self, class_colors):\n",
    "        self.class_colors = class_colors\n",
    "\n",
    "    def __call__(self, img, label):\n",
    "        img = np.array(img) / 255.0  # Normalize grayscale to [0,1]\n",
    "        img = np.expand_dims(img, axis=-1)  # Shape: (28,28,1)\n",
    "\n",
    "        # Get the class-specific color\n",
    "        color = self.class_colors[label]\n",
    "\n",
    "        # Apply the class-specific color to all pixels\n",
    "        colorized_img = img * color  # Element-wise multiplication\n",
    "\n",
    "        return torch.tensor(colorized_img, dtype=torch.float32).permute(2, 0, 1)  # Convert to PyTorch tensor (C, H, W), keeping 3 channels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function to apply transform\n",
    "class CustomFashionMNIST(torchvision.datasets.FashionMNIST):\n",
    "    def __init__(self, *args, transform=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.custom_transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = super().__getitem__(index)  # Get image and label\n",
    "        if self.custom_transform:\n",
    "            img = self.custom_transform(img, label)  # Apply color transform\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with the custom transformation\n",
    "train_dataset = CustomFashionMNIST(\n",
    "    root=directory, train=True, download=True, transform=ClassColorizeTransform(CLASS_COLORS)\n",
    ")\n",
    "\n",
    "# DataLoader for batch processing\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_images(dataset, num_images=8):\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(12, 4))\n",
    "    for i in range(num_images):\n",
    "        image, label = dataset[i]\n",
    "        image = image.permute(1, 2, 0).numpy()  # Convert tensor to numpy image\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].set_title(f\"Class {label}\")\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Show sample colorized images\n",
    "show_images(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train, test\n",
    "from cnn import _2LayerCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [_2LayerCNN().to(device).to(torch.float32) for _ in range(2)]\n",
    "optimizers = [optim.Adam(model.parameters()) for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.298153\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model, optimizer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(models, optimizers):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Works/OoD_Research/ood-experiments/experiments/train.py:42\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, device, train_loader, optimizer, epoch)\u001b[39m\n\u001b[32m     40\u001b[39m output = model(data)\n\u001b[32m     41\u001b[39m loss = criterion(output, target)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m optimizer.step()\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_idx % \u001b[32m100\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Works/OoD_Research/.venv/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Works/OoD_Research/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Works/OoD_Research/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for model, optimizer in zip(models, optimizers):\n",
    "    for epoch in range(1, 5):\n",
    "        train(model, device, train_loader, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_color_set(length):\n",
    "   \n",
    "    # Generate random integers in the range [0, 255] for each color channel\n",
    "    color_set = np.random.randint(0, 256, size=(length, 3))\n",
    "    return color_set / 255.0 # Normalize to [0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize_transform(img, color_set):\n",
    "    # Convert tensor to numpy and remove channel dimension added by ToTensor()\n",
    "    img = np.array(img).squeeze(0)  # Now shape (28, 28)\n",
    "    \n",
    "    # Normalize and expand to (28, 28, 1)\n",
    "    img = img / 255.0\n",
    "    img = np.expand_dims(img, axis=-1)  # Shape: (28, 28, 1)\n",
    "    \n",
    "    # Map to colors\n",
    "    num_colors = len(color_set)\n",
    "    color_indices = (img * (num_colors - 1)).astype(int)\n",
    "    colorized_img = color_set[color_indices]  # Shape: (28, 28, 1, 3)\n",
    "    \n",
    "    # Remove the unnecessary singleton dimension\n",
    "    colorized_img = colorized_img.squeeze(2)  # Shape: (28, 28, 3)\n",
    "    \n",
    "    # Convert to tensor and permute to (C, H, W)\n",
    "    return torch.tensor(colorized_img, dtype=torch.float32).permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random 9 colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_9_colors = generate_random_color_set(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to tensor first\n",
    "    transforms.Lambda(lambda x: colorize_transform(x * 255, random_9_colors))  # Apply colorization\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.FashionMNIST(\n",
    "    directory, train=False, transform=transform)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing the next model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_91361/1492103977.py:3: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  img = np.array(img).squeeze(0)  # Now shape (28, 28)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'dict_values' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtesting the next model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Works/OoD_Research/ood-experiments/experiments/train.py:56\u001b[39m, in \u001b[36mtest\u001b[39m\u001b[34m(model, device, test_loader)\u001b[39m\n\u001b[32m     54\u001b[39m correct = \u001b[32m0\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Works/OoD_Research/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Works/OoD_Research/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Works/OoD_Research/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Works/OoD_Research/.venv/lib/python3.12/site-packages/torchvision/datasets/mnist.py:146\u001b[39m, in \u001b[36mMNIST.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    143\u001b[39m img = Image.fromarray(img.numpy(), mode=\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    149\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Works/OoD_Research/.venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Works/OoD_Research/.venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:479\u001b[39m, in \u001b[36mLambda.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlambd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      1\u001b[39m transform = transforms.Compose([\n\u001b[32m      2\u001b[39m     transforms.ToTensor(),  \u001b[38;5;66;03m# Convert image to tensor first\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     transforms.Lambda(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mcolorize_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLASS_COLORS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Apply colorization\u001b[39;00m\n\u001b[32m      4\u001b[39m ])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mcolorize_transform\u001b[39m\u001b[34m(img, color_set)\u001b[39m\n\u001b[32m     10\u001b[39m num_colors = \u001b[38;5;28mlen\u001b[39m(color_set)\n\u001b[32m     11\u001b[39m color_indices = (img * (num_colors - \u001b[32m1\u001b[39m)).astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m colorized_img = \u001b[43mcolor_set\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolor_indices\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Shape: (28, 28, 1, 3)\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Remove the unnecessary singleton dimension\u001b[39;00m\n\u001b[32m     15\u001b[39m colorized_img = colorized_img.squeeze(\u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# Shape: (28, 28, 3)\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: 'dict_values' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(\"testing the next model\")\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9 colors of Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to tensor first\n",
    "    transforms.Lambda(lambda x: colorize_transform(x * 255, np.array(list(CLASS_COLORS.values())) / 255.0 ))  # Apply colorization\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.FashionMNIST(\n",
    "    directory, train=False, transform=transform)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing the next model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_91361/1492103977.py:3: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  img = np.array(img).squeeze(0)  # Now shape (28, 28)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0356, Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "testing the next model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtesting the next model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Works/OoD_Research/ood-experiments/experiments/train.py:56\u001b[39m, in \u001b[36mtest\u001b[39m\u001b[34m(model, device, test_loader)\u001b[39m\n\u001b[32m     54\u001b[39m correct = \u001b[32m0\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Works/OoD_Research/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Works/OoD_Research/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Works/OoD_Research/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Works/OoD_Research/.venv/lib/python3.12/site-packages/torchvision/datasets/mnist.py:146\u001b[39m, in \u001b[36mMNIST.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    143\u001b[39m img = Image.fromarray(img.numpy(), mode=\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    149\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Works/OoD_Research/.venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Works/OoD_Research/.venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:479\u001b[39m, in \u001b[36mLambda.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlambd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      1\u001b[39m transform = transforms.Compose([\n\u001b[32m      2\u001b[39m     transforms.ToTensor(),  \u001b[38;5;66;03m# Convert image to tensor first\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     transforms.Lambda(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mcolorize_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCLASS_COLORS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m255.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Apply colorization\u001b[39;00m\n\u001b[32m      4\u001b[39m ])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mcolorize_transform\u001b[39m\u001b[34m(img, color_set)\u001b[39m\n\u001b[32m     12\u001b[39m colorized_img = color_set[color_indices]  \u001b[38;5;66;03m# Shape: (28, 28, 1, 3)\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Remove the unnecessary singleton dimension\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m colorized_img = \u001b[43mcolorized_img\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: (28, 28, 3)\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Convert to tensor and permute to (C, H, W)\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.tensor(colorized_img, dtype=torch.float32).permute(\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(\"testing the next model\")\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
